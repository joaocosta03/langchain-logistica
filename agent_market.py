"""
Agente de an√°lise de mercado usando Gemini via LangChain.
"""
import os
import json
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
# Import din√¢mico para compatibilidade entre vers√µes do LangChain
try:
    from langchain.agents import create_agent  # LangChain 1.0+
    _LC_AGENT_API = "create_agent"
except Exception:  # noqa: E722
    create_agent = None  # type: ignore
    try:
        from langchain.agents import create_react_agent, AgentExecutor  # LangChain 0.2.x
        _LC_AGENT_API = "react"
    except Exception:  # noqa: E722
        create_react_agent = None  # type: ignore
        AgentExecutor = None  # type: ignore
        try:
            from langchain.agents import initialize_agent  # LangChain 0.0.x/0.1.x
            _LC_AGENT_API = "initialize"
        except Exception:  # noqa: E722
            initialize_agent = None  # type: ignore
            _LC_AGENT_API = None
# Import compat√≠vel de Tool entre vers√µes do LangChain
try:
    from langchain_core.tools import Tool  # LangChain 0.2+
except Exception:  # noqa: E722
    from langchain.tools import Tool  # Fallback para vers√µes antigas

from callbacks import TAOConsoleLogger
from tools import web_search, calc_cagr, report_refine


# Carrega vari√°veis de ambiente
load_dotenv()


def run_market_agent(topic: str, start_rev: float, end_rev: float, months: float) -> str:
    """
    Executa o agente para um tema de mercado (ex.: 'Blockchain em Log√≠stica').
    
    Args:
        topic: Tema de mercado para an√°lise
        start_rev: Valor inicial para c√°lculo CAGR
        end_rev: Valor final para c√°lculo CAGR
        months: Per√≠odo em meses para c√°lculo CAGR
    
    Returns:
        Relat√≥rio consolidado em 4 par√°grafos
    
    Raises:
        ValueError: Se GEMINI_API_KEY n√£o estiver configurada
    """
    # Validar chave da API
    gemini_key = os.getenv("GEMINI_API_KEY")
    if not gemini_key:
        raise ValueError(
            "GEMINI_API_KEY n√£o encontrada. "
            "Crie um arquivo .env baseado em .env.example e configure suas chaves."
        )
    
    # Instanciar o modelo Gemini
    llm = ChatGoogleGenerativeAI(
        model="gemini-flash-lite-latest",
        temperature=0.2,
        max_output_tokens=1500,
        google_api_key=gemini_key
    )
    
    # Construir ferramentas do LangChain
    tools = [
        Tool(
            name="web_search",
            func=lambda q: json.dumps(web_search(q, num=5, time_period="m6"), ensure_ascii=False, indent=2),
            description=(
                "Busca not√≠cias, relat√≥rios e informa√ß√µes recentes na web sobre um tema espec√≠fico. "
                "Use quando precisar encontrar fontes atualizadas sobre investimentos, crescimento, "
                "tend√™ncias de mercado ou desenvolvimentos tecnol√≥gicos. "
                "Input: string com a consulta de busca. "
                "Output: JSON com lista de resultados contendo title, link, snippet e date."
            )
        ),
        Tool(
            name="calc_cagr",
            func=lambda js: calc_cagr(**json.loads(js)),
            description=(
                "Calcula o CAGR (Compound Annual Growth Rate) anualizado. "
                "Input: JSON string com formato {\"start\": float, \"end\": float, \"months\": float}. "
                "start = valor inicial, end = valor final, months = per√≠odo em meses. "
                "Output: CAGR como decimal (ex.: 0.44 para 44%)."
            )
        ),
        Tool(
            name="report_refine",
            func=report_refine,
            description=(
                "Refina e limpa um texto, removendo espa√ßos extras e normalizando formata√ß√£o. "
                "Use ao final para polir o relat√≥rio. "
                "Input: texto a ser refinado. "
                "Output: texto limpo."
            )
        )
    ]
    
    # Construir prompt detalhado (usado tanto pelo executor quanto no fallback manual)
    prompt = f"""
Voc√™ √© um analista de mercado especializado. Sua tarefa √© produzir um relat√≥rio consolidado sobre o tema: "{topic}".

INSTRU√á√ïES OBRIGAT√ìRIAS:

1. **Buscar Fontes (√∫ltimos 6 meses)**: Use a ferramenta web_search para encontrar not√≠cias ou relat√≥rios publicados nos √öLTIMOS 6 MESES sobre investimentos, crescimento ou desenvolvimentos relacionados a "{topic}".

2. **Selecionar 2 Fontes**: Do resultado da busca, selecione exatamente 2 fontes relevantes que mencionem investimentos, crescimento de mercado ou avan√ßos tecnol√≥gicos.

3. **Citar com Links e Datas**: Para cada fonte selecionada:
   - Inclua o t√≠tulo
   - Inclua o link completo (URL)
   - Inclua a data em formato DD/M√™s/AAAA ou AAAA-MM-DD quando dispon√≠vel
   - Se a data n√£o estiver dispon√≠vel, informe "data n√£o informada"

4. **Calcular CAGR**: Use a ferramenta calc_cagr com os seguintes par√¢metros:
   - start: {start_rev}
   - end: {end_rev}
   - months: {months}
   
   O resultado ser√° um decimal (ex.: 0.44). Converta para percentual com 2 casas decimais (ex.: 44,00%).

5. **Consolidar Relat√≥rio**: Escreva um relat√≥rio em portugu√™s (PT-BR) com EXATAMENTE 4 par√°grafos:
   
   **Par√°grafo 1**: Contexto geral sobre "{topic}", sua relev√¢ncia e tend√™ncias atuais.
   
   **Par√°grafo 2**: Apresente a primeira fonte citando t√≠tulo, link e data. Resuma o que ela diz sobre investimentos/crescimento.
   
   **Par√°grafo 3**: Apresente a segunda fonte citando t√≠tulo, link e data. Resuma o que ela diz sobre investimentos/crescimento.
   
   **Par√°grafo 4**: An√°lise de crescimento: mencione o CAGR calculado (em formato XX,XX%) e conclua sobre o potencial de mercado.

6. **N√£o Inventar Dados**: Use apenas informa√ß√µes das fontes encontradas. N√£o invente n√∫meros ou datas.

7. **Opcional**: Ao final, voc√™ pode usar report_refine para limpar o texto.

Comece agora a an√°lise.
"""
    
    # Construir agente compat√≠vel com a vers√£o instalada do LangChain
    print(f"üîç Usando API do LangChain: {_LC_AGENT_API or 'fallback manual'}\n")
    if _LC_AGENT_API == "create_agent":
        # LangChain 1.0+: create_agent retorna um CompiledStateGraph diretamente execut√°vel
        agent_graph = create_agent(
            model=llm,
            tools=tools,
            debug=False,  # Desliga o debug do LangChain
            system_prompt="Voc√™ √© um analista de mercado especializado."
        )
        # Encapsular o graph em um wrapper para manter compatibilidade com invoke({"input": ...})
        class AgentWrapper:
            def __init__(self, graph, logger):
                self.graph = graph
                self.logger = logger
            def invoke(self, input_dict):
                self.logger.on_chain_start({}, input_dict)
                try:
                    # O create_agent retorna um StateGraph que aceita {"messages": [...]}
                    from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
                    
                    result = self.graph.invoke({"messages": [HumanMessage(content=input_dict["input"])]}, config={"recursion_limit": 50})
                    
                    # Extrair tool calls das mensagens para exibir TAO customizado
                    messages = result.get("messages", [])
                    for i, msg in enumerate(messages):
                        if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
                            for tool_call in msg.tool_calls:
                                self.logger.on_tool_start({"name": tool_call.get("name", "unknown")}, str(tool_call.get("args", {})))
                        
                        elif isinstance(msg, ToolMessage):
                            self.logger.on_tool_end(msg.content if isinstance(msg.content, str) else str(msg.content)[:500])
                    
                    self.logger.on_chain_end({"output": messages[-1].content if messages and hasattr(messages[-1], 'content') else str(result)})
                    return {"output": messages[-1].content if messages and hasattr(messages[-1], 'content') else str(result)}
                except Exception as e:
                    self.logger.on_tool_error(e)
                    raise
        agent = AgentWrapper(agent_graph, TAOConsoleLogger())
    elif _LC_AGENT_API == "react":
        react_agent = create_react_agent(llm=llm, tools=tools)
        agent = AgentExecutor(
            agent=react_agent,
            tools=tools,
            verbose=True,
            max_iterations=8,
            early_stopping_method="generate",
            callbacks=[TAOConsoleLogger()],
            handle_parsing_errors=True
        )
    elif _LC_AGENT_API == "initialize":
        agent = initialize_agent(
            tools=tools,
            llm=llm,
            agent="zero-shot-react-description",
            verbose=True,
            max_iterations=8,
            early_stopping_method="generate",
            callbacks=[TAOConsoleLogger()],
            handle_parsing_errors=True
        )
    else:
        # Fallback manual: orquestra√ß√£o simples com logs TAO
        print("‚ö†Ô∏è  Modo fallback manual ativado (API de agente n√£o dispon√≠vel)\n")
        logger = TAOConsoleLogger()
        logger.on_chain_start({}, {"input": prompt})

        # Action: web_search
        logger.on_tool_start({"name": "web_search"}, topic)
        try:
            search_results = web_search(f"{topic} investimentos crescimento", num=5, time_period="m6")
            logger.on_tool_end(json.dumps(search_results, ensure_ascii=False)[:500])
        except Exception as e:
            logger.on_tool_error(e)
            raise

        # Pedir ao LLM para escolher 2 fontes e resumir
        selection_prompt = (
            "Voc√™ recebeu resultados de busca em JSON. Selecione exatamente 2 fontes relevantes, "
            "citando t√≠tulo, link e data, e produza um breve resumo de 2-3 frases para cada. "
            "Se a data n√£o estiver dispon√≠vel, escreva 'data n√£o informada'.\n\n"
            f"RESULTADOS:\n{json.dumps(search_results, ensure_ascii=False, indent=2)}\n\n"
            "Responda em JSON com o formato: {\"fontes\": [ {\"titulo\": ..., \"link\": ..., \"data\": ..., \"resumo\": ...}, {...} ]}"
        )
        selection = llm.invoke(selection_prompt)

        # Action: calc_cagr
        logger.on_tool_start({"name": "calc_cagr"}, json.dumps({"start": start_rev, "end": end_rev, "months": months}))
        try:
            cagr_value = calc_cagr(start=start_rev, end=end_rev, months=months)
            logger.on_tool_end(str(cagr_value))
        except Exception as e:
            logger.on_tool_error(e)
            raise

        # Escrever relat√≥rio final com 4 par√°grafos
        try:
            data = selection.content if hasattr(selection, "content") else str(selection)
            final_prompt = (
                "Com base nas duas fontes selecionadas (JSON a seguir) e no CAGR informado, "
                "escreva um relat√≥rio em portugu√™s (PT-BR) com EXATAMENTE 4 par√°grafos: \n"
                "Par√°grafo 1: contexto geral do tema.\n"
                "Par√°grafo 2: apresente a primeira fonte (t√≠tulo, link, data) e um resumo.\n"
                "Par√°grafo 3: apresente a segunda fonte (t√≠tulo, link, data) e um resumo.\n"
                "Par√°grafo 4: an√°lise de crescimento mencionando o CAGR em formato XX,XX% e conclus√£o.\n\n"
                f"FONTES (JSON):\n{data}\n\n"
                f"CAGR decimal: {cagr_value}. Converta para percentual com 2 casas."
            )
            report = llm.invoke(final_prompt)
            text = report.content if hasattr(report, "content") else str(report)
            text = report_refine(text)
            logger.on_chain_end({"output": text})
            return text
        except Exception as e:
            logger.on_tool_error(e)
            raise
    
    
    # Executar o agente
    try:
        result = agent.invoke({"input": prompt})
        return result["output"]
    except Exception as e:
        return f"Erro ao executar o agente: {str(e)}"

